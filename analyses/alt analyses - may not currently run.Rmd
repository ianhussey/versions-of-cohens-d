---
title: "Meta-analyses"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    toc: yes
    toc_float: yes
---

https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/tdunpaired

```{r include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies 

```{r}

library(tidyverse)
library(readxl)
library(janitor)
library(effsize)
library(metafor)
library(knitr)
library(kableExtra)
library(broom)
library(esci)

```

# Get data

```{r}

data_processed_after_exclusions <- read_csv("../../study 4/data/processed/data_processed.csv") %>%
  # do exclusions
  filter(exclude == FALSE) %>%
  # recode conditions
  mutate(condition = str_replace(condition, "modelled__", "Modelled "),
         condition = str_replace(condition, "traditional__", "Traditional "),
         condition = str_replace(condition, "read_plus_rewrite_from_1st_person", "+ rewrite"),
         condition = str_replace(condition, "read_plus_instruction_to_PT", "+ PT"),
         condition = str_replace(condition, "read", ""))

```

# Sample sizes

```{r}

data_processed_after_exclusions %>%
  count() %>%
  arrange() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

# Differences between pre and post

```{r}

fit_mv_meta <- function(data){
  rma.mv(yi     = es,
         V      = variance, 
         random = ~ 1 | condition,
         slab   = paste0(condition, ": ", scale),
         data   = data)
}

```

```{r}

data_reshaped <- 
  full_join(data_processed_after_exclusions %>%
              select(id, condition, 
                     mass = mass_pre_sum_score,
                     mhs_g = mhs_g_pre_sum_score,
                     mhs_l = mhs_l_pre_sum_score) %>%
              gather(scale, pre, c("mass", "mhs_g", "mhs_l")),
            data_processed_after_exclusions %>%
              select(id, 
                     mass = mass_post_sum_score,
                     mhs_g = mhs_g_post_sum_score,
                     mhs_l = mhs_l_post_sum_score) %>%
              gather(scale, post, c("mass", "mhs_g", "mhs_l")),
            by = c("id", "scale")) %>%
  drop_na() %>%
  mutate(scale = case_when(scale == "mass" ~ "MASS",
                           scale == "mhs_g" ~ "MHS-G",
                           scale == "mhs_l" ~ "MHS-L"))

```

## Classic Cohen's d (ds)

```{r}

cohens_d <- function(data, invert = FALSE){
  # NB this requires that pre be the second level of the timepoint factor - ensure this is changed in wrangling
  require(effsize)
  fit <- effsize::cohen.d(score ~ timepoint,
                          paired = FALSE,
                          pooled = TRUE,
                          data = data)
  
  results <- 
    tibble(es       = fit$estimate,
           ci_lower = fit$conf.int[1],
           ci_upper = fit$conf.int[2]) %>%
    mutate(variance = ((ci_upper - ci_lower) / (1.96*2))^2)
  
  return(results)
}

results_cohens_d <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(condition, scale) %>%
  do(cohens_d(.)) 

fit_cohens_d <- fit_mv_meta(results_cohens_d)

forest(fit_cohens_d, xlab = "Cohen's d")

```

## Classic Hedge's g

```{r}

hedges_g <- function(data, invert = FALSE){
  # NB this requires that pre be the second level of the timepoint factor - ensure this is changed in wrangling
  require(effsize)
  fit <- effsize::cohen.d(score ~ timepoint,
                          paired = FALSE,
                          pooled = TRUE,
                          hedges.correction = TRUE,
                          data = data)
  
  results <- 
    tibble(es       = fit$estimate,
           ci_lower = fit$conf.int[1],
           ci_upper = fit$conf.int[2]) %>%
    mutate(variance = ((ci_upper - ci_lower) / (1.96*2))^2)
  
  return(results)
}

results_hedges_g <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(condition, scale) %>%
  do(hedges_g(.)) 

fit_hedges_g <- fit_mv_meta(results_hedges_g)

forest(fit_hedges_g, xlab = "Hedges' g")

```

## Glass' $\Delta$

SDs are larger at post, providing a motivation for Delta over d.

```{r}

data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(scale, timepoint) %>%
  summarize(sd = sd(score))

```

```{r}

results_glass_delta <- data_reshaped %>%
  mutate(diff = post - pre) %>%
  group_by(condition, scale) %>%
  summarize(es = mean(diff)/sd(pre),
            n = n(),
            .groups = "drop") %>%
  mutate(ci_lower = psych::cohen.d.ci(es, n1 = n)[1, "lower"],
         ci_upper = psych::cohen.d.ci(es, n1 = n)[1, "upper"],
         variance = ((ci_upper - ci_lower) / (1.96*2))^2)

fit_glass_delta <- fit_mv_meta(results_glass_delta)

forest(fit_glass_delta, xlab = "Glass's Delta")

```

## Glass' $\Delta$ post

SDs are larger at post, providing a motivation for Delta over d.

```{r}

results_glass_delta_post <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "pre", "post")) %>%
  group_by(condition, scale) %>%
  do(glass_delta(.)) %>%
  mutate(es = es*-1,
         ci_temp = ci_lower * -1,
         ci_lower = ci_upper * -1,
         ci_upper = ci_temp) %>%
  select(-ci_temp)

fit_glass_delta_post <- fit_mv_meta(results_glass_delta_post)

forest(fit_glass_delta_post, xlab = "Glass' Delta post")

```

## Cohen's dz

```{r}

results_cohens_dz <- data_reshaped %>%
  mutate(diff = post - pre) %>%
  group_by(condition, scale) %>%
  summarize(es = mean(diff)/sd(diff),
            n = n()) %>%
  mutate(ci_lower = psych::cohen.d.ci(es, n1 = n)[1, "lower"],
         ci_upper = psych::cohen.d.ci(es, n1 = n)[1, "upper"],
         variance = ((ci_upper - ci_lower) / (1.96*2))^2)

fit_cohens_dz <- fit_mv_meta(results_cohens_dz)

forest(fit_cohens_dz, xlab = "Cohen's dz")

```

## Paired Cohen's drm

### effsize package

I dont fully understand the internal method

```{r}

paired_cohens_d <- function(data, invert = FALSE){
  # NB this requires that pre be the second level of the timepoint factor - ensure this is changed in wrangling
  require(effsize)
  fit <- effsize::cohen.d(score ~ timepoint | Subject(id),
                          paired = TRUE,
                          pooled = TRUE,
                          data = data)

  results <- 
    tibble(es       = fit$estimate,
           ci_lower = fit$conf.int[1],
           ci_upper = fit$conf.int[2]) %>%
    mutate(variance = ((ci_upper - ci_lower) / (1.96*2))^2)
  
  return(results)
}

results_within_subjects_cohens_d <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(condition, scale) %>%
  do(paired_cohens_d(.))

fit_within_subjects_cohens_d <- fit_mv_meta(results_within_subjects_cohens_d)

forest(fit_within_subjects_cohens_d, xlab = "Within-subjects Cohen's d")

```

### drm from lakens 2013 NEEDED

```{r}



```


## dav RECOMMENDED

This is the only one that is all of (1) known internal workings, (2) good estimation width becuase it takes within subject non-independence into account, and (3) has congruent magnitude interpretations with Cohen's ds / can be meta analyzed with it / can't be accused of effect size inflation. 

Lakens 2013, cummings 2016, esci package.

```{r}

# devtools::install_github("rcalinjageman/esci")
library(esci)

cohens_dav <- function(data){
  require(esci)
  
  fit <- 
    esci::estimateStandardizedMeanDifference(m1 = data$mean_post, 
                                             m2 = data$mean_pre, 
                                             s1 = data$sd_post, 
                                             s2 = data$sd_pre, 
                                             n1 = data$n, 
                                             n2 = data$n,
                                             r  = data$r, 
                                             conf.level = .95,
                                             paired = TRUE) 
  
  return(tibble(es = fit$cohend,
                ci_lower = fit$cohend.low,
                ci_upper = fit$cohend.high,
                r  = fit$r))
} 

results_esci_msd <- data_reshaped %>%
  gather(timepoint, score, c(pre, post)) %>%
  group_by(scale, condition, timepoint) %>%
  summarize(mean = mean(score),
            sd = sd(score),
            n = n(),
            .groups = "drop") %>%
  pivot_wider(names_from = "timepoint",
              values_from = c("mean", "sd")) 

results_esci_r <- data_reshaped %>%
  select(scale, condition, pre, post) %>%
  group_by(scale, condition) %>%
  summarize(r = cor(pre, post))

results_cohens_dav <- 
  full_join(results_esci_msd, results_esci_r, by = c("scale", "condition")) %>%
  group_by(scale, condition) %>%
  do(cohens_dav(.)) %>%
  mutate(variance = ((ci_upper - ci_lower) / (1.96*2))^2)
  
fit_cohens_dav <- fit_mv_meta(results_cohens_dav)

forest(fit_cohens_dav, xlab = "Cohen's dav")

```

## Cohen's dt

### dt from Westfall - check

```{r}

results_cohens_dt <- data_reshaped %>%
  group_by(condition, scale) %>%
  do(tidy(t.test(.$post, .$pre,  
                 paired = TRUE,
                 var.equal = TRUE))) %>%
  select(condition, scale, t_stat = statistic, df = parameter) %>%
  mutate(n = df + 1,
         es = t_stat * sqrt(2/n)) %>%
  mutate(ci_lower = psych::cohen.d.ci(es, n1 = n)[1, "lower"],
         ci_upper = psych::cohen.d.ci(es, n1 = n)[1, "upper"],
         variance = ((ci_upper - ci_lower) / (1.96*2))^2)

fit_cohens_dt <- fit_mv_meta(results_cohens_dt)

forest(fit_cohens_dt, xlab = "Cohen's dz")

```

### dt from Lakens 2013

```{r}

results_cohens_dt <- data_reshaped %>%
  group_by(condition, scale) %>%
  do(tidy(t.test(.$post, .$pre,  
                 paired = TRUE,
                 var.equal = TRUE))) %>%
  select(condition, scale, t_stat = statistic, df = parameter) %>%
  mutate(n = df + 1,
         es = t_stat / sqrt(n)) %>%
  mutate(ci_lower = psych::cohen.d.ci(es, n1 = n)[1, "lower"],
         ci_upper = psych::cohen.d.ci(es, n1 = n)[1, "upper"],
         variance = ((ci_upper - ci_lower) / (1.96*2))^2)

fit_cohens_dt <- fit_mv_meta(results_cohens_dt)

forest(fit_cohens_dt, xlab = "Cohen's dz")

```

## Cohen's dr- remove?

```{r}

options(contrasts = c("contr.helmert","contr.poly"))
library(lme4)

fit_cohens_dr <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(condition, scale) %>%
  lmer(score ~ timepoint + (timepoint || condition/id), data = .)

summary(fit_cohens_dr)

```

-0.7178/1.216 = -0.5902961

## Probability of superiority

```{r}

#' Bootstrapped Ruscio's A with 95 percent CIs and standard error
#'
#' This function bootstraps confidence intervals for Ruscio's A effect size (2008). 
#' Code adapted from adapted from John Ruscio's original implementation of his metric: https://ruscio.pages.tcnj.edu/quantitative-methods-program-code/
#' @param data data
#' @param variable continuous variable
#' @param group dichotomous group
#' @param value1 assignment of group 1
#' @param value2 assignment of group 2
#' @param Conf.Level 1 - alpha value (e.g., .95).
#' @param seed seed value for reproducability
#' @param B Number of boostrapped resamples
#' @param adjust_ceiling Should Ruscio's A estimates of 0 and 1 be adjusted so that they can be converted to finite odds ratios? This is done by rescoring a single data point as being was inferior to a single second data point between the conditions. Ie., it uses the best granularity allowed by the data, as more data points will result in a more extreme possible values of A.
#' @return ruscios_A_estimate Ruscio's A.
#' @return ruscios_A_se Standard error of bootstrapped Ruscio's A values.
#' @return ruscios_A_ci_lwr Lower 95% bootstrapped confidence interval via the BCA method
#' @return ruscios_A_ci_upr Upper 95% bootstrapped confidence interval via the BCA method
#' @export
#' @examples
#' ruscios_A_boot(data = simulated_data, variable = "Score", group = "Condition", value1 = "B", value2 = "A")
#'
ruscios_A_boot <- function(data, variable, group, value1 = 1, value2 = 0, 
                           B = 2000, Conf.Level = .95, seed = 1,
                           adjust_ceiling = TRUE) {
  
  # Fast calculation of the A statistic
  ruscios_A_function <- function(x, y) {
    nx <- length(x)
    ny <- length(y)
    rx <- sum(rank(c(x, y))[1:nx])
    A = (rx / nx - (nx + 1) / 2) / ny
    # if adjust_ceiling == TRUE & A == 0 or 1, rescore it as if a single data point was inferior to a single second data point between conditions. 
    # Ie., use the lowest granularity allowed by the data for rescoring. More data points will result in a higher adjusted A.
    if(adjust_ceiling == TRUE & A == 1){
      A <- ruscios_A_function(c(rep(4, length(x)-1), 2), c(rep(1, length(y)-1), 3))
    } else if(adjust_ceiling == TRUE & A == 0){
      A <- 1 - ruscios_A_function(c(rep(4, length(x)-1), 2), c(rep(1, length(y)-1), 3))
    }
    return(A)
  }
  
  # Ensure data is a data frame (e.g., not a tbl_data)
  data <- as.data.frame(data)
  
  # Select the observations for group 1
  x <- data[data[[group]] == value1, variable]
  
  # Select the observations for group 2
  y <- data[data[[group]] == value2, variable]
  
  
  # initialize variables
  set.seed(seed)
  nx <- length(x)
  ny <- length(y)
  A.obs <- ruscios_A_function(x, y)
  Alpha <- 1 - Conf.Level
  CI.Lower <- CI.Upper <- pi
  
  # perform bootstrap to generate B values of A
  BS.Values <- rep(0, B)
  for (i in 1:B) {
    BS.Values[i] <- ruscios_A_function(sample(x, replace = T), sample(y, replace = TRUE))
  }
  
  BS.Values <- sort(BS.Values)
  
  # if all bootstrap samples yield same value for A, use it for both ends of CI
  if (min(BS.Values) == max(BS.Values)) {
    CI.Lower <- CI.Upper <- BS.Values[1]
  }
  
  # if sample value not within range of bootstrap values, revert to percentile CI
  if ((A.obs < min(BS.Values)) | (A.obs > max(BS.Values))) {
    CI.Lower <- BS.Values[round((Alpha / 2) * B)]
    CI.Upper <- BS.Values[round((1 - Alpha / 2) * B)]
  }
  
  # otherwise, use BCA CI
  if ((CI.Lower == pi) & (CI.Upper == pi)) {
    # calculate bias-correction and acceleration parameters (z0 and a)
    z0 <- qnorm(mean(BS.Values < A.obs))
    
    jk <- rep(0, (nx + ny))
    for (i in 1:nx) {
      jk[i] <- ruscios_A_function(x[-i], y)
    }
    
    for (i in 1:ny) {
      jk[nx + i] <- ruscios_A_function(x, y[-i])
    }
    
    Diff <- mean(jk) - jk
    a <- sum(Diff ^ 3) / (6 * (sum(Diff ^ 2)) ^ 1.5)
    
    # adjust location of endpoints
    Alpha1 <- pnorm(z0 + (z0 + qnorm(Alpha/2)) / (1 - a * (z0 + qnorm(Alpha/2))))
    Alpha2 <- pnorm(z0 + (z0 - qnorm(Alpha/2)) / (1 - a * (z0 - qnorm(Alpha/2))))
    
    # if either endpoint undefined, replace it with value for percentile CI
    if (is.na(Alpha1)) {Alpha1 <- Alpha / 2}
    if (is.na(Alpha2)) {Alpha2 <- 1 - Alpha / 2}
    
    if (round(Alpha1 * B) < 1) {CI.Lower <- BS.Values[1]}
    else {
      CI.Lower <- BS.Values[round(Alpha1 * B)]
      CI.Upper <- BS.Values[round(Alpha2 * B)]	
    }
  }
  
  # return A, SE of A, lower limit of CI, upper limit of CI
  results <- data.frame(ruscios_A        = round(A.obs,         3),
                        ruscios_A_se     = round(sd(BS.Values), 3),
                        ruscios_A_ci_lwr = round(CI.Lower,      3),
                        ruscios_A_ci_upr = round(CI.Upper,      3))
  
  return(results)
}


results_pos <- data_reshaped %>%
  gather(timepoint, score, c("pre", "post")) %>%
  mutate(timepoint = fct_relevel(timepoint, "post", "pre")) %>%
  group_by(condition, scale) %>%
  do(ruscios_A_boot(variable = "score",
                    group = "timepoint",
                    data = .,
                    value1 = "post",
                    value2 = "pre",
                    adjust_ceiling = FALSE)) %>%
  dplyr::ungroup() %>%
  mutate(variance = ruscios_A_se^2) %>%
  select(condition, scale, es = ruscios_A, ci_lower = ruscios_A_ci_lwr, ci_upper = ruscios_A_ci_upr, variance)

fit_pos <- fit_mv_meta(results_pos)

forest(fit_pos, xlab = "Probability of superiority", refline = 0.50)

```



```{r}

pos_to_ds <- function(cles) {
  d <- qnorm(cles) * sqrt(2)
  return(d)
}

fit_pos_to_ds <- results_pos %>%
  mutate(es = pos_to_ds(es),
         ci_lower = pos_to_ds(ci_lower),
         ci_upper = pos_to_ds(ci_upper),
         variance = ((ci_upper - ci_lower) / (1.96*2))^2) %>%
  fit_mv_meta()

forest(fit_pos_to_ds, xlab = "Probability of superiority converted to Cohen's ds")

```


## POS within

my own implementation by bootstrapping between participants

```{r}

library(rsample)
library(broom)
library(purrr)

# create bootstraps using out of bag method. makes a df with values that are collapsed dfs.
boots <- data_reshaped %>%
  group_by(condition, scale) %>% 
  bootstraps(times = 2000)

analyze_bootstrap <- function(split) {
  analysis(split) %>%
    group_by(condition, scale) %>% 
    summarize(pos = mean(pre < post)) %>%
    ungroup()
}

# apply to each bootstrap
results_pos_within_to_ds <- boots %>% 
  mutate(boot_pos = map(splits, analyze_bootstrap))%>% 
  unnest(boot_pos) %>% 
  select(-splits) %>%
  group_by(condition, scale) %>% 
  summarize(ci_lower = quantile(pos, 0.025),
            ci_upper = quantile(pos, 0.975)) %>%
  full_join(data_reshaped %>%
              group_by(condition, scale) %>%
              summarize(es = mean(pre < post)),
            by = c("condition", "scale")) %>%
  mutate(es = pos_to_ds(es),
         ci_lower = pos_to_ds(ci_lower),
         ci_upper = pos_to_ds(ci_upper),
         variance = ((ci_upper - ci_lower) / (1.96*2))^2) %>%
  fit_mv_meta()

forest(results_pos_within_to_ds, xlab = "Within-subjects probability of superiority converted to Cohen's ds")

```

## Comparisons

```{r}

results_effect_sizes <- 
  tibble(method = c("Cohen's d",
                    "Hedge's g",
                    "Glass delta",
                    "Within-subjects Cohen's d",
                    "Cohen's dt",
                    "Cohen's dz",
                    "Cohen's dav"),
         es = c(fit_cohens_d$b,
                fit_hedges_g$b,
                fit_glass_delta$b,
                fit_within_subjects_cohens_d$b,
                fit_cohens_dt$b,
                fit_cohens_dz$b,
                fit_cohens_dav$b),
         ci_lower = c(fit_cohens_d$ci.lb,
                      fit_hedges_g$ci.lb,
                      fit_glass_delta$ci.lb,
                      fit_within_subjects_cohens_d$ci.lb,
                      fit_cohens_dt$ci.lb,
                      fit_cohens_dz$ci.lb,
                      fit_cohens_dav$ci.lb),
         ci_upper = c(fit_cohens_d$ci.ub,
                      fit_hedges_g$ci.ub,
                      fit_glass_delta$ci.ub,
                      fit_within_subjects_cohens_d$ci.ub,
                      fit_cohens_dt$ci.ub,
                      fit_cohens_dz$ci.ub,
                      fit_cohens_dav$ci.ub)) %>%
  mutate(method = fct_relevel(method, 
                              "Cohen's d",
                              "Hedge's g",
                              "Glass delta",
                              "Within-subjects Cohen's d",
                              "Cohen's dt",
                              "Cohen's dz",
                              "Cohen's dav"))

ggplot(results_effect_sizes, aes(method, es)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_point() +
  coord_flip() +
  theme_linedraw() +
  xlab("Method") +
  ylab("Meta effect size")

```

